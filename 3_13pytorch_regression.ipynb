{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6R+qjxMBYgMhCaPExDyXN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zelal-Eizaldeen/deeplearning_course/blob/main/3_13pytorch_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In this programming example, we will implement a network to solve a regression problem using **PyTorch**."
      ],
      "metadata": {
        "id": "2_C6yozBKCW7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use California housing dataset. Each training example will refer to a house in California and the input variables represent information about this house, like its size or its location. And then the output value is the price of the house, and the idea here is to try to train the network to, given a number of input variables that describes the house, to predict what is the cost of this house."
      ],
      "metadata": {
        "id": "G8S3q01nK9yA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The MIT License (MIT)\n",
        "Copyright (c) 2021 NVIDIA\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
        "this software and associated documentation files (the \"Software\"), to deal in\n",
        "the Software without restriction, including without limitation the rights to\n",
        "use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
        "the Software, and to permit persons to whom the Software is furnished to do so,\n",
        "subject to the following conditions:\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
        "FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
        "COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
        "IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
        "CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "AmtecOQyLMct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset is not included in PyTorch, so we will import it from scikit-learn. We are going to train with a batch size of 128 for 256 epochs."
      ],
      "metadata": {
        "id": "vj4kBCCfLLwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "EPOCHS = 256\n",
        "BATCH_SIZE = 128"
      ],
      "metadata": {
        "id": "NgN8stxALT08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will import the dataset here using the fetch California housing, separate the data and targets. We will then split this into a training and a test part using this function from scikit-learn. We'll say that we want to use 20% of the dataset as a test dataset, and the rest for training."
      ],
      "metadata": {
        "id": "oFZkod-JLZLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read dataset and split into train and test.\n",
        "california_housing = fetch_california_housing()\n",
        "data = california_housing.get('data')\n",
        "target = california_housing.get('target')\n",
        "raw_x_train, raw_x_test, y_train, y_test = train_test_split(\n",
        "    data, target, test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "0FuD4PdOLhDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to then convert some of this into the type of the data types that we want to use. So we want to use a float32, because that is what we're using for this model. If we don't do that, we will get an error message later."
      ],
      "metadata": {
        "id": "GCtZAvUxLtub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to same precision as model.\n",
        "raw_x_train = raw_x_train.astype(np.float32)\n",
        "raw_x_test = raw_x_test.astype(np.float32)\n",
        "y_train = y_train.astype(np.float32)\n",
        "y_test = y_test.astype(np.float32)\n",
        "y_train = np.reshape(y_train, (-1, 1))\n",
        "y_test = np.reshape(y_test, (-1, 1))\n"
      ],
      "metadata": {
        "id": "iibInxtNLw8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to standardize the data."
      ],
      "metadata": {
        "id": "XY-dpdBrL4rl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize the data.\n",
        "x_mean = np.mean(raw_x_train, axis=0)\n",
        "x_stddev = np.std(raw_x_train, axis=0)\n",
        "x_train = (raw_x_train - x_mean) / x_stddev\n",
        "x_test = (raw_x_test - x_mean) / x_stddev"
      ],
      "metadata": {
        "id": "EdSOcX3uL660"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And then we **create the dataset objects**."
      ],
      "metadata": {
        "id": "hP2ZFN9cML8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Dataset objects.\n",
        "trainset = TensorDataset(torch.from_numpy(x_train),\n",
        "                         torch.from_numpy(y_train))\n",
        "testset = TensorDataset(torch.from_numpy(x_test),\n",
        "                        torch.from_numpy(y_test))"
      ],
      "metadata": {
        "id": "WPP-ZkyDMebc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it's time to **create the model**. We specify the first layer being a linear layer with eight inputs and 32 neurons. We have a ReLU activation function for that. And then the output layer, its inputs should match the output from the previous layer for PyTorch. And then the output layer has a single neuron, because we are predicting a single value,"
      ],
      "metadata": {
        "id": "xpiIEOB4MiKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model.\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(8, 32),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(32, 1)\n",
        ")\n"
      ],
      "metadata": {
        "id": "kgvl1hg0MmcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To initialize the weights, we use **the Xavier initialization function**, and we do that for the linear layers. We will use the **Adam optimizer**, and the **mean squared error loss functions**. MSE loss is what you want to use when you do a regression problem and **using a linear activation function in the output layer.**"
      ],
      "metadata": {
        "id": "zHmalKxQMzcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize weights.\n",
        "for module in model.modules():\n",
        "    if isinstance(module, nn.Linear):\n",
        "        nn.init.xavier_uniform_(module.weight)\n",
        "        nn.init.constant_(module.bias, 0.0)\n",
        "\n",
        "# Loss function and optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "loss_function = nn.MSELoss()"
      ],
      "metadata": {
        "id": "XXEurTB1NJyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Utilites.py"
      ],
      "metadata": {
        "id": "FR8tek_iN-yQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yQrSUUIPAA7",
        "outputId": "9d0e9f2e-5ffc-4b33-cf85-bf9d38a0fd24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, importlib"
      ],
      "metadata": {
        "id": "AxPuFvEGPNJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ROOT = \"your_directory_to_utilities.py\"\n",
        "if ROOT not in sys.path:\n",
        "    sys.path.insert(0, ROOT)\n",
        "\n",
        "# Clear cache and import\n",
        "sys.modules.pop(\"utilities\", None)\n",
        "import utilities; importlib.reload(utilities)\n",
        "\n",
        "from utilities import train_model  # should work now"
      ],
      "metadata": {
        "id": "ms_XrETOiHM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from utilities import train_model\n"
      ],
      "metadata": {
        "id": "NCUH302ecwZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To **train the model**, we've actually refactored the code here a little bit so we don't have to write this training function over and over again. So I've lifted that out into a utilities.py, where we have a function that is containing the training function."
      ],
      "metadata": {
        "id": "2sR8k2lhNNC5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Where we have the outer training loops with number of epochs and then going through all the batches, moving to the GPU, doing the forward pass, accumulate metrics, and then backward pass."
      ],
      "metadata": {
        "id": "VTGIFgGDN23j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One thing we have changed here is that we're also given, we have a parameter here that says **what metric do we want to print.** So when we look at the classification problem, we want to look at accuracy, how many of the examples did we classify correctly. For a regression problem, that doesn't make any sense because we will not predict exactly the right value, so we want to look at some other **metric** **as how far away are we from the true value**, so we want to **use mean absolute error instead**."
      ],
      "metadata": {
        "id": "EM1ftRD-OBuJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So this training_model function can take either accuracy as input or mean absolute error, and depending on which one you give, it will calculate that metric a little bit differently **where we either check if they're equal or we're accumulating the absolute error**. We can just call it with train model. We pass the model input to it, we say if we have a GPU or not, number of epochs, batch size, the training and test dataset, what optimizer we will use, what loss function, and then finally, what metric."
      ],
      "metadata": {
        "id": "ith_MH0VOWO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model.\n",
        "utilities.train_model(model, device, EPOCHS, BATCH_SIZE, trainset, testset,\n",
        "            optimizer, loss_function, 'mae')"
      ],
      "metadata": {
        "id": "UDDQTwgYOzWu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c3f4f1e-4315-4fd8-c872-5e39b27a6de0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/256 loss: 2.5985 - mae: 1.2328 - val_loss: 1.0307 - val_mae: 0.7184\n",
            "Epoch 2/256 loss: 0.8076 - mae: 0.6481 - val_loss: 0.7120 - val_mae: 0.5942\n",
            "Epoch 3/256 loss: 0.6543 - mae: 0.5801 - val_loss: 0.6172 - val_mae: 0.5524\n",
            "Epoch 4/256 loss: 0.5738 - mae: 0.5412 - val_loss: 0.5629 - val_mae: 0.5198\n",
            "Epoch 5/256 loss: 0.5124 - mae: 0.5108 - val_loss: 0.4879 - val_mae: 0.4905\n",
            "Epoch 6/256 loss: 0.4711 - mae: 0.4890 - val_loss: 0.4560 - val_mae: 0.4724\n",
            "Epoch 7/256 loss: 0.4401 - mae: 0.4726 - val_loss: 0.4642 - val_mae: 0.4608\n",
            "Epoch 8/256 loss: 0.4214 - mae: 0.4613 - val_loss: 0.4166 - val_mae: 0.4491\n",
            "Epoch 9/256 loss: 0.4084 - mae: 0.4547 - val_loss: 0.4349 - val_mae: 0.4473\n",
            "Epoch 10/256 loss: 0.4006 - mae: 0.4502 - val_loss: 0.4069 - val_mae: 0.4414\n",
            "Epoch 11/256 loss: 0.3937 - mae: 0.4475 - val_loss: 0.3938 - val_mae: 0.4377\n",
            "Epoch 12/256 loss: 0.3886 - mae: 0.4437 - val_loss: 0.4048 - val_mae: 0.4362\n",
            "Epoch 13/256 loss: 0.3855 - mae: 0.4416 - val_loss: 0.3878 - val_mae: 0.4358\n",
            "Epoch 14/256 loss: 0.3823 - mae: 0.4397 - val_loss: 0.4057 - val_mae: 0.4335\n",
            "Epoch 15/256 loss: 0.3796 - mae: 0.4374 - val_loss: 0.4137 - val_mae: 0.4324\n",
            "Epoch 16/256 loss: 0.3777 - mae: 0.4368 - val_loss: 0.3982 - val_mae: 0.4256\n",
            "Epoch 17/256 loss: 0.3758 - mae: 0.4351 - val_loss: 0.3710 - val_mae: 0.4222\n",
            "Epoch 18/256 loss: 0.3711 - mae: 0.4322 - val_loss: 0.3824 - val_mae: 0.4238\n",
            "Epoch 19/256 loss: 0.3686 - mae: 0.4306 - val_loss: 0.3958 - val_mae: 0.4249\n",
            "Epoch 20/256 loss: 0.3656 - mae: 0.4284 - val_loss: 0.3728 - val_mae: 0.4246\n",
            "Epoch 21/256 loss: 0.3636 - mae: 0.4274 - val_loss: 0.3808 - val_mae: 0.4187\n",
            "Epoch 22/256 loss: 0.3615 - mae: 0.4255 - val_loss: 0.3840 - val_mae: 0.4179\n",
            "Epoch 23/256 loss: 0.3599 - mae: 0.4242 - val_loss: 0.3772 - val_mae: 0.4150\n",
            "Epoch 24/256 loss: 0.3573 - mae: 0.4227 - val_loss: 0.3608 - val_mae: 0.4147\n",
            "Epoch 25/256 loss: 0.3549 - mae: 0.4210 - val_loss: 0.3650 - val_mae: 0.4145\n",
            "Epoch 26/256 loss: 0.3523 - mae: 0.4192 - val_loss: 0.3537 - val_mae: 0.4107\n",
            "Epoch 27/256 loss: 0.3503 - mae: 0.4175 - val_loss: 0.3471 - val_mae: 0.4090\n",
            "Epoch 28/256 loss: 0.3497 - mae: 0.4171 - val_loss: 0.3466 - val_mae: 0.4073\n",
            "Epoch 29/256 loss: 0.3468 - mae: 0.4151 - val_loss: 0.3562 - val_mae: 0.4091\n",
            "Epoch 30/256 loss: 0.3442 - mae: 0.4139 - val_loss: 0.3591 - val_mae: 0.4062\n",
            "Epoch 31/256 loss: 0.3424 - mae: 0.4120 - val_loss: 0.3450 - val_mae: 0.4029\n",
            "Epoch 32/256 loss: 0.3397 - mae: 0.4104 - val_loss: 0.3582 - val_mae: 0.4019\n",
            "Epoch 33/256 loss: 0.3403 - mae: 0.4101 - val_loss: 0.3840 - val_mae: 0.4001\n",
            "Epoch 34/256 loss: 0.3390 - mae: 0.4082 - val_loss: 0.3529 - val_mae: 0.4022\n",
            "Epoch 35/256 loss: 0.3350 - mae: 0.4066 - val_loss: 0.3491 - val_mae: 0.4044\n",
            "Epoch 36/256 loss: 0.3322 - mae: 0.4053 - val_loss: 0.3356 - val_mae: 0.4014\n",
            "Epoch 37/256 loss: 0.3300 - mae: 0.4033 - val_loss: 0.3363 - val_mae: 0.4039\n",
            "Epoch 38/256 loss: 0.3289 - mae: 0.4024 - val_loss: 0.3361 - val_mae: 0.3972\n",
            "Epoch 39/256 loss: 0.3272 - mae: 0.4012 - val_loss: 0.3388 - val_mae: 0.3935\n",
            "Epoch 40/256 loss: 0.3264 - mae: 0.4004 - val_loss: 0.3248 - val_mae: 0.3916\n",
            "Epoch 41/256 loss: 0.3253 - mae: 0.3995 - val_loss: 0.3241 - val_mae: 0.3910\n",
            "Epoch 42/256 loss: 0.3235 - mae: 0.3982 - val_loss: 0.3564 - val_mae: 0.3931\n",
            "Epoch 43/256 loss: 0.3230 - mae: 0.3966 - val_loss: 0.3367 - val_mae: 0.3935\n",
            "Epoch 44/256 loss: 0.3205 - mae: 0.3958 - val_loss: 0.3318 - val_mae: 0.3958\n",
            "Epoch 45/256 loss: 0.3215 - mae: 0.3958 - val_loss: 0.3409 - val_mae: 0.3966\n",
            "Epoch 46/256 loss: 0.3196 - mae: 0.3951 - val_loss: 0.3229 - val_mae: 0.3922\n",
            "Epoch 47/256 loss: 0.3174 - mae: 0.3936 - val_loss: 0.3250 - val_mae: 0.3854\n",
            "Epoch 48/256 loss: 0.3195 - mae: 0.3945 - val_loss: 0.3287 - val_mae: 0.3906\n",
            "Epoch 49/256 loss: 0.3167 - mae: 0.3932 - val_loss: 0.3193 - val_mae: 0.3844\n",
            "Epoch 50/256 loss: 0.3146 - mae: 0.3922 - val_loss: 0.3197 - val_mae: 0.3852\n",
            "Epoch 51/256 loss: 0.3135 - mae: 0.3912 - val_loss: 0.3189 - val_mae: 0.3823\n",
            "Epoch 52/256 loss: 0.3135 - mae: 0.3909 - val_loss: 0.3168 - val_mae: 0.3865\n",
            "Epoch 53/256 loss: 0.3119 - mae: 0.3897 - val_loss: 0.3188 - val_mae: 0.3859\n",
            "Epoch 54/256 loss: 0.3119 - mae: 0.3895 - val_loss: 0.3224 - val_mae: 0.3856\n",
            "Epoch 55/256 loss: 0.3139 - mae: 0.3904 - val_loss: 0.3248 - val_mae: 0.3840\n",
            "Epoch 56/256 loss: 0.3138 - mae: 0.3890 - val_loss: 0.3820 - val_mae: 0.3850\n",
            "Epoch 57/256 loss: 0.3117 - mae: 0.3883 - val_loss: 0.3177 - val_mae: 0.3856\n",
            "Epoch 58/256 loss: 0.3087 - mae: 0.3876 - val_loss: 0.3164 - val_mae: 0.3822\n",
            "Epoch 59/256 loss: 0.3092 - mae: 0.3873 - val_loss: 0.3111 - val_mae: 0.3792\n",
            "Epoch 60/256 loss: 0.3080 - mae: 0.3872 - val_loss: 0.3150 - val_mae: 0.3886\n",
            "Epoch 61/256 loss: 0.3069 - mae: 0.3856 - val_loss: 0.3215 - val_mae: 0.3858\n",
            "Epoch 62/256 loss: 0.3065 - mae: 0.3854 - val_loss: 0.3111 - val_mae: 0.3800\n",
            "Epoch 63/256 loss: 0.3067 - mae: 0.3852 - val_loss: 0.3113 - val_mae: 0.3803\n",
            "Epoch 64/256 loss: 0.3098 - mae: 0.3856 - val_loss: 0.3169 - val_mae: 0.3817\n",
            "Epoch 65/256 loss: 0.3052 - mae: 0.3844 - val_loss: 0.3526 - val_mae: 0.3862\n",
            "Epoch 66/256 loss: 0.3077 - mae: 0.3850 - val_loss: 0.3363 - val_mae: 0.3822\n",
            "Epoch 67/256 loss: 0.3070 - mae: 0.3843 - val_loss: 0.3467 - val_mae: 0.3861\n",
            "Epoch 68/256 loss: 0.3065 - mae: 0.3847 - val_loss: 0.3122 - val_mae: 0.3845\n",
            "Epoch 69/256 loss: 0.3034 - mae: 0.3833 - val_loss: 0.3115 - val_mae: 0.3777\n",
            "Epoch 70/256 loss: 0.3026 - mae: 0.3828 - val_loss: 0.3109 - val_mae: 0.3812\n",
            "Epoch 71/256 loss: 0.3022 - mae: 0.3819 - val_loss: 0.3099 - val_mae: 0.3795\n",
            "Epoch 72/256 loss: 0.3018 - mae: 0.3816 - val_loss: 0.3104 - val_mae: 0.3814\n",
            "Epoch 73/256 loss: 0.3018 - mae: 0.3821 - val_loss: 0.3083 - val_mae: 0.3818\n",
            "Epoch 74/256 loss: 0.3010 - mae: 0.3814 - val_loss: 0.3073 - val_mae: 0.3758\n",
            "Epoch 75/256 loss: 0.3045 - mae: 0.3827 - val_loss: 0.3085 - val_mae: 0.3815\n",
            "Epoch 76/256 loss: 0.3014 - mae: 0.3819 - val_loss: 0.3062 - val_mae: 0.3745\n",
            "Epoch 77/256 loss: 0.3010 - mae: 0.3814 - val_loss: 0.3095 - val_mae: 0.3743\n",
            "Epoch 78/256 loss: 0.3000 - mae: 0.3806 - val_loss: 0.3118 - val_mae: 0.3758\n",
            "Epoch 79/256 loss: 0.3010 - mae: 0.3810 - val_loss: 0.3457 - val_mae: 0.3840\n",
            "Epoch 80/256 loss: 0.3037 - mae: 0.3810 - val_loss: 0.3105 - val_mae: 0.3810\n",
            "Epoch 81/256 loss: 0.3024 - mae: 0.3808 - val_loss: 0.3325 - val_mae: 0.3837\n",
            "Epoch 82/256 loss: 0.3040 - mae: 0.3800 - val_loss: 0.3245 - val_mae: 0.3871\n",
            "Epoch 83/256 loss: 0.2995 - mae: 0.3801 - val_loss: 0.3068 - val_mae: 0.3751\n",
            "Epoch 84/256 loss: 0.2981 - mae: 0.3791 - val_loss: 0.3061 - val_mae: 0.3783\n",
            "Epoch 85/256 loss: 0.2978 - mae: 0.3793 - val_loss: 0.3033 - val_mae: 0.3739\n",
            "Epoch 86/256 loss: 0.2983 - mae: 0.3789 - val_loss: 0.3048 - val_mae: 0.3754\n",
            "Epoch 87/256 loss: 0.2981 - mae: 0.3786 - val_loss: 0.3117 - val_mae: 0.3781\n",
            "Epoch 88/256 loss: 0.2986 - mae: 0.3788 - val_loss: 0.3136 - val_mae: 0.3798\n",
            "Epoch 89/256 loss: 0.2971 - mae: 0.3782 - val_loss: 0.3087 - val_mae: 0.3735\n",
            "Epoch 90/256 loss: 0.2972 - mae: 0.3782 - val_loss: 0.3131 - val_mae: 0.3741\n",
            "Epoch 91/256 loss: 0.2973 - mae: 0.3779 - val_loss: 0.3168 - val_mae: 0.3758\n",
            "Epoch 92/256 loss: 0.2976 - mae: 0.3776 - val_loss: 0.3160 - val_mae: 0.3763\n",
            "Epoch 93/256 loss: 0.2979 - mae: 0.3774 - val_loss: 0.3266 - val_mae: 0.3809\n",
            "Epoch 94/256 loss: 0.3014 - mae: 0.3774 - val_loss: 0.3306 - val_mae: 0.3819\n",
            "Epoch 95/256 loss: 0.2975 - mae: 0.3771 - val_loss: 0.3314 - val_mae: 0.3777\n",
            "Epoch 96/256 loss: 0.2978 - mae: 0.3775 - val_loss: 0.3202 - val_mae: 0.3816\n",
            "Epoch 97/256 loss: 0.2970 - mae: 0.3771 - val_loss: 0.3251 - val_mae: 0.3778\n",
            "Epoch 98/256 loss: 0.2959 - mae: 0.3765 - val_loss: 0.3045 - val_mae: 0.3764\n",
            "Epoch 99/256 loss: 0.2947 - mae: 0.3759 - val_loss: 0.3053 - val_mae: 0.3807\n",
            "Epoch 100/256 loss: 0.2964 - mae: 0.3774 - val_loss: 0.3050 - val_mae: 0.3727\n",
            "Epoch 101/256 loss: 0.2960 - mae: 0.3757 - val_loss: 0.3030 - val_mae: 0.3761\n",
            "Epoch 102/256 loss: 0.2943 - mae: 0.3756 - val_loss: 0.3145 - val_mae: 0.3789\n",
            "Epoch 103/256 loss: 0.2948 - mae: 0.3754 - val_loss: 0.3074 - val_mae: 0.3776\n",
            "Epoch 104/256 loss: 0.2983 - mae: 0.3773 - val_loss: 0.3287 - val_mae: 0.3770\n",
            "Epoch 105/256 loss: 0.2961 - mae: 0.3752 - val_loss: 0.3178 - val_mae: 0.3768\n",
            "Epoch 106/256 loss: 0.2953 - mae: 0.3757 - val_loss: 0.3045 - val_mae: 0.3744\n",
            "Epoch 107/256 loss: 0.2933 - mae: 0.3750 - val_loss: 0.3018 - val_mae: 0.3776\n",
            "Epoch 108/256 loss: 0.2931 - mae: 0.3747 - val_loss: 0.3037 - val_mae: 0.3779\n",
            "Epoch 109/256 loss: 0.2933 - mae: 0.3747 - val_loss: 0.3033 - val_mae: 0.3750\n",
            "Epoch 110/256 loss: 0.2925 - mae: 0.3747 - val_loss: 0.3032 - val_mae: 0.3731\n",
            "Epoch 111/256 loss: 0.2923 - mae: 0.3744 - val_loss: 0.3051 - val_mae: 0.3746\n",
            "Epoch 112/256 loss: 0.2940 - mae: 0.3747 - val_loss: 0.3072 - val_mae: 0.3730\n",
            "Epoch 113/256 loss: 0.2930 - mae: 0.3744 - val_loss: 0.3049 - val_mae: 0.3766\n",
            "Epoch 114/256 loss: 0.2918 - mae: 0.3738 - val_loss: 0.3063 - val_mae: 0.3807\n",
            "Epoch 115/256 loss: 0.2919 - mae: 0.3738 - val_loss: 0.3013 - val_mae: 0.3735\n",
            "Epoch 116/256 loss: 0.2957 - mae: 0.3746 - val_loss: 0.3026 - val_mae: 0.3721\n",
            "Epoch 117/256 loss: 0.2913 - mae: 0.3728 - val_loss: 0.3074 - val_mae: 0.3758\n",
            "Epoch 118/256 loss: 0.2915 - mae: 0.3735 - val_loss: 0.3299 - val_mae: 0.3774\n",
            "Epoch 119/256 loss: 0.2924 - mae: 0.3738 - val_loss: 0.3271 - val_mae: 0.3764\n",
            "Epoch 120/256 loss: 0.2934 - mae: 0.3737 - val_loss: 0.3118 - val_mae: 0.3821\n",
            "Epoch 121/256 loss: 0.2924 - mae: 0.3740 - val_loss: 0.3274 - val_mae: 0.3772\n",
            "Epoch 122/256 loss: 0.2972 - mae: 0.3743 - val_loss: 0.3137 - val_mae: 0.3754\n",
            "Epoch 123/256 loss: 0.2914 - mae: 0.3727 - val_loss: 0.3044 - val_mae: 0.3753\n",
            "Epoch 124/256 loss: 0.2937 - mae: 0.3735 - val_loss: 0.3161 - val_mae: 0.3720\n",
            "Epoch 125/256 loss: 0.2922 - mae: 0.3729 - val_loss: 0.3122 - val_mae: 0.3762\n",
            "Epoch 126/256 loss: 0.2898 - mae: 0.3730 - val_loss: 0.3008 - val_mae: 0.3709\n",
            "Epoch 127/256 loss: 0.2893 - mae: 0.3712 - val_loss: 0.3049 - val_mae: 0.3791\n",
            "Epoch 128/256 loss: 0.2901 - mae: 0.3731 - val_loss: 0.3027 - val_mae: 0.3753\n",
            "Epoch 129/256 loss: 0.2905 - mae: 0.3724 - val_loss: 0.3141 - val_mae: 0.3811\n",
            "Epoch 130/256 loss: 0.2927 - mae: 0.3744 - val_loss: 0.3154 - val_mae: 0.3829\n",
            "Epoch 131/256 loss: 0.2923 - mae: 0.3736 - val_loss: 0.3062 - val_mae: 0.3749\n",
            "Epoch 132/256 loss: 0.2904 - mae: 0.3720 - val_loss: 0.3049 - val_mae: 0.3786\n",
            "Epoch 133/256 loss: 0.2891 - mae: 0.3719 - val_loss: 0.3116 - val_mae: 0.3712\n",
            "Epoch 134/256 loss: 0.2893 - mae: 0.3718 - val_loss: 0.3046 - val_mae: 0.3818\n",
            "Epoch 135/256 loss: 0.2889 - mae: 0.3718 - val_loss: 0.3125 - val_mae: 0.3772\n",
            "Epoch 136/256 loss: 0.2903 - mae: 0.3722 - val_loss: 0.3145 - val_mae: 0.3773\n",
            "Epoch 137/256 loss: 0.2899 - mae: 0.3718 - val_loss: 0.3078 - val_mae: 0.3766\n",
            "Epoch 138/256 loss: 0.2920 - mae: 0.3728 - val_loss: 0.3042 - val_mae: 0.3765\n",
            "Epoch 139/256 loss: 0.2903 - mae: 0.3727 - val_loss: 0.3066 - val_mae: 0.3760\n",
            "Epoch 140/256 loss: 0.2915 - mae: 0.3720 - val_loss: 0.3199 - val_mae: 0.3756\n",
            "Epoch 141/256 loss: 0.2885 - mae: 0.3710 - val_loss: 0.3134 - val_mae: 0.3786\n",
            "Epoch 142/256 loss: 0.2924 - mae: 0.3730 - val_loss: 0.3274 - val_mae: 0.3749\n",
            "Epoch 143/256 loss: 0.2894 - mae: 0.3708 - val_loss: 0.3173 - val_mae: 0.3749\n",
            "Epoch 144/256 loss: 0.2878 - mae: 0.3718 - val_loss: 0.3106 - val_mae: 0.3728\n",
            "Epoch 145/256 loss: 0.2892 - mae: 0.3708 - val_loss: 0.3022 - val_mae: 0.3766\n",
            "Epoch 146/256 loss: 0.2886 - mae: 0.3724 - val_loss: 0.3172 - val_mae: 0.3779\n",
            "Epoch 147/256 loss: 0.2896 - mae: 0.3710 - val_loss: 0.3113 - val_mae: 0.3734\n",
            "Epoch 148/256 loss: 0.2883 - mae: 0.3708 - val_loss: 0.3000 - val_mae: 0.3711\n",
            "Epoch 149/256 loss: 0.2877 - mae: 0.3710 - val_loss: 0.3178 - val_mae: 0.3760\n",
            "Epoch 150/256 loss: 0.2875 - mae: 0.3706 - val_loss: 0.3204 - val_mae: 0.3707\n",
            "Epoch 151/256 loss: 0.2926 - mae: 0.3714 - val_loss: 0.3322 - val_mae: 0.3748\n",
            "Epoch 152/256 loss: 0.2951 - mae: 0.3723 - val_loss: 0.3004 - val_mae: 0.3708\n",
            "Epoch 153/256 loss: 0.2870 - mae: 0.3699 - val_loss: 0.2999 - val_mae: 0.3725\n",
            "Epoch 154/256 loss: 0.2863 - mae: 0.3698 - val_loss: 0.3014 - val_mae: 0.3759\n",
            "Epoch 155/256 loss: 0.2867 - mae: 0.3704 - val_loss: 0.2999 - val_mae: 0.3700\n",
            "Epoch 156/256 loss: 0.2859 - mae: 0.3694 - val_loss: 0.3009 - val_mae: 0.3749\n",
            "Epoch 157/256 loss: 0.2876 - mae: 0.3702 - val_loss: 0.2975 - val_mae: 0.3727\n",
            "Epoch 158/256 loss: 0.2877 - mae: 0.3703 - val_loss: 0.2990 - val_mae: 0.3708\n",
            "Epoch 159/256 loss: 0.2872 - mae: 0.3706 - val_loss: 0.2991 - val_mae: 0.3697\n",
            "Epoch 160/256 loss: 0.2860 - mae: 0.3694 - val_loss: 0.3066 - val_mae: 0.3715\n",
            "Epoch 161/256 loss: 0.2918 - mae: 0.3710 - val_loss: 0.2957 - val_mae: 0.3716\n",
            "Epoch 162/256 loss: 0.2864 - mae: 0.3696 - val_loss: 0.2989 - val_mae: 0.3698\n",
            "Epoch 163/256 loss: 0.2861 - mae: 0.3698 - val_loss: 0.2978 - val_mae: 0.3714\n",
            "Epoch 164/256 loss: 0.2855 - mae: 0.3698 - val_loss: 0.2961 - val_mae: 0.3686\n",
            "Epoch 165/256 loss: 0.2866 - mae: 0.3699 - val_loss: 0.3013 - val_mae: 0.3705\n",
            "Epoch 166/256 loss: 0.2899 - mae: 0.3700 - val_loss: 0.3118 - val_mae: 0.3721\n",
            "Epoch 167/256 loss: 0.2886 - mae: 0.3699 - val_loss: 0.3044 - val_mae: 0.3724\n",
            "Epoch 168/256 loss: 0.2862 - mae: 0.3691 - val_loss: 0.3097 - val_mae: 0.3733\n",
            "Epoch 169/256 loss: 0.2870 - mae: 0.3698 - val_loss: 0.2999 - val_mae: 0.3710\n",
            "Epoch 170/256 loss: 0.2847 - mae: 0.3685 - val_loss: 0.2980 - val_mae: 0.3691\n",
            "Epoch 171/256 loss: 0.2864 - mae: 0.3691 - val_loss: 0.3090 - val_mae: 0.3746\n",
            "Epoch 172/256 loss: 0.2858 - mae: 0.3693 - val_loss: 0.3000 - val_mae: 0.3787\n",
            "Epoch 173/256 loss: 0.2867 - mae: 0.3692 - val_loss: 0.2965 - val_mae: 0.3711\n",
            "Epoch 174/256 loss: 0.2864 - mae: 0.3689 - val_loss: 0.3139 - val_mae: 0.3734\n",
            "Epoch 175/256 loss: 0.2947 - mae: 0.3721 - val_loss: 0.2975 - val_mae: 0.3715\n",
            "Epoch 176/256 loss: 0.2853 - mae: 0.3692 - val_loss: 0.2957 - val_mae: 0.3707\n",
            "Epoch 177/256 loss: 0.2849 - mae: 0.3689 - val_loss: 0.2969 - val_mae: 0.3726\n",
            "Epoch 178/256 loss: 0.2853 - mae: 0.3683 - val_loss: 0.3100 - val_mae: 0.3708\n",
            "Epoch 179/256 loss: 0.2885 - mae: 0.3691 - val_loss: 0.3058 - val_mae: 0.3740\n",
            "Epoch 180/256 loss: 0.2853 - mae: 0.3696 - val_loss: 0.2976 - val_mae: 0.3695\n",
            "Epoch 181/256 loss: 0.2851 - mae: 0.3679 - val_loss: 0.2982 - val_mae: 0.3727\n",
            "Epoch 182/256 loss: 0.2887 - mae: 0.3698 - val_loss: 0.3159 - val_mae: 0.3702\n",
            "Epoch 183/256 loss: 0.2856 - mae: 0.3685 - val_loss: 0.3078 - val_mae: 0.3732\n",
            "Epoch 184/256 loss: 0.2848 - mae: 0.3682 - val_loss: 0.2989 - val_mae: 0.3750\n",
            "Epoch 185/256 loss: 0.2846 - mae: 0.3686 - val_loss: 0.2997 - val_mae: 0.3704\n",
            "Epoch 186/256 loss: 0.2852 - mae: 0.3691 - val_loss: 0.3075 - val_mae: 0.3708\n",
            "Epoch 187/256 loss: 0.2859 - mae: 0.3691 - val_loss: 0.3311 - val_mae: 0.3785\n",
            "Epoch 188/256 loss: 0.2890 - mae: 0.3690 - val_loss: 0.3077 - val_mae: 0.3719\n",
            "Epoch 189/256 loss: 0.2860 - mae: 0.3700 - val_loss: 0.3054 - val_mae: 0.3679\n",
            "Epoch 190/256 loss: 0.2845 - mae: 0.3682 - val_loss: 0.3115 - val_mae: 0.3762\n",
            "Epoch 191/256 loss: 0.2848 - mae: 0.3680 - val_loss: 0.2978 - val_mae: 0.3695\n",
            "Epoch 192/256 loss: 0.2854 - mae: 0.3686 - val_loss: 0.3045 - val_mae: 0.3704\n",
            "Epoch 193/256 loss: 0.2840 - mae: 0.3677 - val_loss: 0.2983 - val_mae: 0.3717\n",
            "Epoch 194/256 loss: 0.2864 - mae: 0.3688 - val_loss: 0.3037 - val_mae: 0.3735\n",
            "Epoch 195/256 loss: 0.2853 - mae: 0.3681 - val_loss: 0.3069 - val_mae: 0.3699\n",
            "Epoch 196/256 loss: 0.2841 - mae: 0.3676 - val_loss: 0.3223 - val_mae: 0.3720\n",
            "Epoch 197/256 loss: 0.2887 - mae: 0.3684 - val_loss: 0.3224 - val_mae: 0.3698\n",
            "Epoch 198/256 loss: 0.2838 - mae: 0.3675 - val_loss: 0.3027 - val_mae: 0.3692\n",
            "Epoch 199/256 loss: 0.2833 - mae: 0.3670 - val_loss: 0.2977 - val_mae: 0.3718\n",
            "Epoch 200/256 loss: 0.2859 - mae: 0.3680 - val_loss: 0.3121 - val_mae: 0.3743\n",
            "Epoch 201/256 loss: 0.3035 - mae: 0.3706 - val_loss: 0.2933 - val_mae: 0.3672\n",
            "Epoch 202/256 loss: 0.2842 - mae: 0.3678 - val_loss: 0.2945 - val_mae: 0.3700\n",
            "Epoch 203/256 loss: 0.2829 - mae: 0.3669 - val_loss: 0.3000 - val_mae: 0.3715\n",
            "Epoch 204/256 loss: 0.2836 - mae: 0.3678 - val_loss: 0.2996 - val_mae: 0.3684\n",
            "Epoch 205/256 loss: 0.2829 - mae: 0.3673 - val_loss: 0.3027 - val_mae: 0.3679\n",
            "Epoch 206/256 loss: 0.2838 - mae: 0.3677 - val_loss: 0.3144 - val_mae: 0.3724\n",
            "Epoch 207/256 loss: 0.2849 - mae: 0.3674 - val_loss: 0.3162 - val_mae: 0.3716\n",
            "Epoch 208/256 loss: 0.2835 - mae: 0.3669 - val_loss: 0.3184 - val_mae: 0.3718\n",
            "Epoch 209/256 loss: 0.2841 - mae: 0.3679 - val_loss: 0.3323 - val_mae: 0.3696\n",
            "Epoch 210/256 loss: 0.2874 - mae: 0.3671 - val_loss: 0.3708 - val_mae: 0.3778\n",
            "Epoch 211/256 loss: 0.2887 - mae: 0.3687 - val_loss: 0.2952 - val_mae: 0.3687\n",
            "Epoch 212/256 loss: 0.2825 - mae: 0.3668 - val_loss: 0.2957 - val_mae: 0.3693\n",
            "Epoch 213/256 loss: 0.2826 - mae: 0.3667 - val_loss: 0.2970 - val_mae: 0.3749\n",
            "Epoch 214/256 loss: 0.2827 - mae: 0.3671 - val_loss: 0.2939 - val_mae: 0.3668\n",
            "Epoch 215/256 loss: 0.2824 - mae: 0.3662 - val_loss: 0.2964 - val_mae: 0.3710\n",
            "Epoch 216/256 loss: 0.2841 - mae: 0.3673 - val_loss: 0.2987 - val_mae: 0.3701\n",
            "Epoch 217/256 loss: 0.2839 - mae: 0.3674 - val_loss: 0.2990 - val_mae: 0.3704\n",
            "Epoch 218/256 loss: 0.2833 - mae: 0.3668 - val_loss: 0.2990 - val_mae: 0.3695\n",
            "Epoch 219/256 loss: 0.2824 - mae: 0.3665 - val_loss: 0.2970 - val_mae: 0.3732\n",
            "Epoch 220/256 loss: 0.2824 - mae: 0.3671 - val_loss: 0.2935 - val_mae: 0.3688\n",
            "Epoch 221/256 loss: 0.2833 - mae: 0.3684 - val_loss: 0.2948 - val_mae: 0.3706\n",
            "Epoch 222/256 loss: 0.2825 - mae: 0.3667 - val_loss: 0.2961 - val_mae: 0.3673\n",
            "Epoch 223/256 loss: 0.2855 - mae: 0.3672 - val_loss: 0.3020 - val_mae: 0.3701\n",
            "Epoch 224/256 loss: 0.2828 - mae: 0.3670 - val_loss: 0.2949 - val_mae: 0.3654\n",
            "Epoch 225/256 loss: 0.2840 - mae: 0.3672 - val_loss: 0.3010 - val_mae: 0.3755\n",
            "Epoch 226/256 loss: 0.2827 - mae: 0.3660 - val_loss: 0.3015 - val_mae: 0.3674\n",
            "Epoch 227/256 loss: 0.2842 - mae: 0.3665 - val_loss: 0.3133 - val_mae: 0.3742\n",
            "Epoch 228/256 loss: 0.2857 - mae: 0.3673 - val_loss: 0.3113 - val_mae: 0.3699\n",
            "Epoch 229/256 loss: 0.2860 - mae: 0.3674 - val_loss: 0.3052 - val_mae: 0.3707\n",
            "Epoch 230/256 loss: 0.2842 - mae: 0.3659 - val_loss: 0.2992 - val_mae: 0.3738\n",
            "Epoch 231/256 loss: 0.2819 - mae: 0.3666 - val_loss: 0.2964 - val_mae: 0.3682\n",
            "Epoch 232/256 loss: 0.2828 - mae: 0.3665 - val_loss: 0.3354 - val_mae: 0.3708\n",
            "Epoch 233/256 loss: 0.2859 - mae: 0.3678 - val_loss: 0.3989 - val_mae: 0.3717\n",
            "Epoch 234/256 loss: 0.2847 - mae: 0.3665 - val_loss: 0.2968 - val_mae: 0.3728\n",
            "Epoch 235/256 loss: 0.2820 - mae: 0.3662 - val_loss: 0.2938 - val_mae: 0.3686\n",
            "Epoch 236/256 loss: 0.2818 - mae: 0.3674 - val_loss: 0.2983 - val_mae: 0.3784\n",
            "Epoch 237/256 loss: 0.2814 - mae: 0.3659 - val_loss: 0.3051 - val_mae: 0.3781\n",
            "Epoch 238/256 loss: 0.2825 - mae: 0.3660 - val_loss: 0.2992 - val_mae: 0.3702\n",
            "Epoch 239/256 loss: 0.2820 - mae: 0.3673 - val_loss: 0.2938 - val_mae: 0.3701\n",
            "Epoch 240/256 loss: 0.2826 - mae: 0.3668 - val_loss: 0.3025 - val_mae: 0.3682\n",
            "Epoch 241/256 loss: 0.2821 - mae: 0.3666 - val_loss: 0.2987 - val_mae: 0.3696\n",
            "Epoch 242/256 loss: 0.2819 - mae: 0.3662 - val_loss: 0.2936 - val_mae: 0.3703\n",
            "Epoch 243/256 loss: 0.2807 - mae: 0.3655 - val_loss: 0.2960 - val_mae: 0.3724\n",
            "Epoch 244/256 loss: 0.2815 - mae: 0.3664 - val_loss: 0.3098 - val_mae: 0.3679\n",
            "Epoch 245/256 loss: 0.2845 - mae: 0.3659 - val_loss: 0.3054 - val_mae: 0.3742\n",
            "Epoch 246/256 loss: 0.2839 - mae: 0.3668 - val_loss: 0.2995 - val_mae: 0.3709\n",
            "Epoch 247/256 loss: 0.2887 - mae: 0.3679 - val_loss: 0.2936 - val_mae: 0.3690\n",
            "Epoch 248/256 loss: 0.2865 - mae: 0.3666 - val_loss: 0.3438 - val_mae: 0.3750\n",
            "Epoch 249/256 loss: 0.2844 - mae: 0.3666 - val_loss: 0.4140 - val_mae: 0.3736\n",
            "Epoch 250/256 loss: 0.2862 - mae: 0.3673 - val_loss: 0.2950 - val_mae: 0.3706\n",
            "Epoch 251/256 loss: 0.2815 - mae: 0.3655 - val_loss: 0.2935 - val_mae: 0.3695\n",
            "Epoch 252/256 loss: 0.2816 - mae: 0.3658 - val_loss: 0.2939 - val_mae: 0.3696\n",
            "Epoch 253/256 loss: 0.2812 - mae: 0.3658 - val_loss: 0.3032 - val_mae: 0.3713\n",
            "Epoch 254/256 loss: 0.2822 - mae: 0.3663 - val_loss: 0.3059 - val_mae: 0.3666\n",
            "Epoch 255/256 loss: 0.2820 - mae: 0.3655 - val_loss: 0.2964 - val_mae: 0.3722\n",
            "Epoch 256/256 loss: 0.2810 - mae: 0.3655 - val_loss: 0.2967 - val_mae: 0.3689\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.36554122317669, 0.3688814357826204]"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that we got a loss of 0.28, and then a validation loss of 0.297. We will now also use **this trained model to do some predictions**. So we can take the test dataset, and provide them to the device, and then apply the model to it. So this will do a prediction for all our examples in the test dataset, and then we will just print out the first three of them, and we will also print out what the actual label is. So we can see here the prediction and the true value, and we can see that they are somewhat similar to each other,"
      ],
      "metadata": {
        "id": "DqG0qnaPi3Dg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print first 3 predictions.\n",
        "inputs = torch.from_numpy(x_test)\n",
        "inputs = inputs.to(device)\n",
        "outputs = model(inputs)\n",
        "for i in range(0, 3):\n",
        "    print('Prediction: %4.2f' % outputs.data[i].item(),\n",
        "         ', true value: %4.2f' % y_test[i].item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0m1hUiZzjRJb",
        "outputId": "84573d5e-b8b4-4bdb-898e-7c489fa581a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: 1.53 , true value: 1.37\n",
            "Prediction: 2.50 , true value: 2.41\n",
            "Prediction: 1.34 , true value: 2.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modified Version"
      ],
      "metadata": {
        "id": "D02LJeV-kt63"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " So let's look at how we can then modify this network to be a little bit better as well. We will have a deeper network."
      ],
      "metadata": {
        "id": "C0UuHfloklmq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "But if you see the **definition of the model**, you can see that we have not only two layers, but we have, **this is the input layer, which is now, instead of 32 neurons, it's 256 neurons, and then we have another hidden layer, which also has 256 neurons.**\n",
        "- Then when I tried that first, I saw that **the training error went down much more than the test error due to overfitting,** so we also then added some **dropout layers here as regularization to try to reduce the amount of overfitting.**"
      ],
      "metadata": {
        "id": "_NBtJKlOnI0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model.\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(8, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(256, 256),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 1)\n",
        ")"
      ],
      "metadata": {
        "id": "OEujnl2YnM8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see now that after finishing"
      ],
      "metadata": {
        "id": "92B7R8mYnoua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize weights.\n",
        "for module in model.modules():\n",
        "    if isinstance(module, nn.Linear):\n",
        "        nn.init.xavier_uniform_(module.weight)\n",
        "        nn.init.constant_(module.bias, 0.0)\n",
        "\n",
        "# Loss function and optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "loss_function = nn.MSELoss()\n",
        "\n",
        "# Train model.\n",
        "train_model(model, device, EPOCHS, BATCH_SIZE, trainset, testset,\n",
        "            optimizer, loss_function, 'mae')\n",
        "\n",
        "# Print first 3 predictions.\n",
        "inputs = torch.from_numpy(x_test)\n",
        "inputs = inputs.to(device)\n",
        "outputs = model(inputs)\n",
        "for i in range(0, 3):\n",
        "    print('Prediction: %4.2f' % outputs.data[i].item(),\n",
        "         ', true value: %4.2f' % y_test[i].item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajJpQBg9ns96",
        "outputId": "5717e8a1-6772-49db-82c0-bb02a8e9a8dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/256 loss: 1.2168 - mae: 0.7010 - val_loss: 0.4743 - val_mae: 0.4871\n",
            "Epoch 2/256 loss: 0.5147 - mae: 0.5146 - val_loss: 0.3868 - val_mae: 0.4351\n",
            "Epoch 3/256 loss: 0.4563 - mae: 0.4821 - val_loss: 0.3994 - val_mae: 0.4223\n",
            "Epoch 4/256 loss: 0.4390 - mae: 0.4720 - val_loss: 0.3825 - val_mae: 0.4189\n",
            "Epoch 5/256 loss: 0.4196 - mae: 0.4613 - val_loss: 0.6780 - val_mae: 0.4210\n",
            "Epoch 6/256 loss: 0.4270 - mae: 0.4540 - val_loss: 0.3643 - val_mae: 0.4021\n",
            "Epoch 7/256 loss: 0.3996 - mae: 0.4449 - val_loss: 0.3372 - val_mae: 0.3999\n",
            "Epoch 8/256 loss: 0.4082 - mae: 0.4453 - val_loss: 0.3584 - val_mae: 0.3981\n",
            "Epoch 9/256 loss: 0.3891 - mae: 0.4377 - val_loss: 0.3191 - val_mae: 0.3830\n",
            "Epoch 10/256 loss: 0.3789 - mae: 0.4315 - val_loss: 0.3209 - val_mae: 0.3849\n",
            "Epoch 11/256 loss: 0.3678 - mae: 0.4259 - val_loss: 0.3976 - val_mae: 0.3812\n",
            "Epoch 12/256 loss: 0.3607 - mae: 0.4209 - val_loss: 0.3264 - val_mae: 0.3864\n",
            "Epoch 13/256 loss: 0.3578 - mae: 0.4195 - val_loss: 0.3200 - val_mae: 0.3699\n",
            "Epoch 14/256 loss: 0.3511 - mae: 0.4157 - val_loss: 0.2992 - val_mae: 0.3662\n",
            "Epoch 15/256 loss: 0.3480 - mae: 0.4131 - val_loss: 0.2956 - val_mae: 0.3678\n",
            "Epoch 16/256 loss: 0.3392 - mae: 0.4067 - val_loss: 0.3351 - val_mae: 0.3679\n",
            "Epoch 17/256 loss: 0.3477 - mae: 0.4100 - val_loss: 0.3019 - val_mae: 0.3660\n",
            "Epoch 18/256 loss: 0.3331 - mae: 0.4039 - val_loss: 0.3500 - val_mae: 0.3628\n",
            "Epoch 19/256 loss: 0.3386 - mae: 0.4037 - val_loss: 0.3115 - val_mae: 0.3704\n",
            "Epoch 20/256 loss: 0.3316 - mae: 0.4000 - val_loss: 0.3027 - val_mae: 0.3616\n",
            "Epoch 21/256 loss: 0.3251 - mae: 0.3957 - val_loss: 0.3125 - val_mae: 0.3574\n",
            "Epoch 22/256 loss: 0.3244 - mae: 0.3965 - val_loss: 0.2845 - val_mae: 0.3578\n",
            "Epoch 23/256 loss: 0.3240 - mae: 0.3948 - val_loss: 0.3245 - val_mae: 0.3568\n",
            "Epoch 24/256 loss: 0.3551 - mae: 0.3920 - val_loss: 0.3009 - val_mae: 0.3579\n",
            "Epoch 25/256 loss: 0.3182 - mae: 0.3904 - val_loss: 0.3087 - val_mae: 0.3634\n",
            "Epoch 26/256 loss: 0.3228 - mae: 0.3928 - val_loss: 0.3291 - val_mae: 0.3555\n",
            "Epoch 27/256 loss: 0.3198 - mae: 0.3898 - val_loss: 0.2782 - val_mae: 0.3474\n",
            "Epoch 28/256 loss: 0.3126 - mae: 0.3866 - val_loss: 0.2810 - val_mae: 0.3624\n",
            "Epoch 29/256 loss: 0.3147 - mae: 0.3866 - val_loss: 0.2764 - val_mae: 0.3493\n",
            "Epoch 30/256 loss: 0.3058 - mae: 0.3833 - val_loss: 0.2773 - val_mae: 0.3512\n",
            "Epoch 31/256 loss: 0.3062 - mae: 0.3824 - val_loss: 0.2785 - val_mae: 0.3479\n",
            "Epoch 32/256 loss: 0.3061 - mae: 0.3814 - val_loss: 0.2790 - val_mae: 0.3451\n",
            "Epoch 33/256 loss: 0.3076 - mae: 0.3819 - val_loss: 0.2701 - val_mae: 0.3475\n",
            "Epoch 34/256 loss: 0.3003 - mae: 0.3773 - val_loss: 0.2734 - val_mae: 0.3511\n",
            "Epoch 35/256 loss: 0.2968 - mae: 0.3769 - val_loss: 0.2791 - val_mae: 0.3510\n",
            "Epoch 36/256 loss: 0.2999 - mae: 0.3770 - val_loss: 0.2811 - val_mae: 0.3551\n",
            "Epoch 37/256 loss: 0.3010 - mae: 0.3776 - val_loss: 0.2713 - val_mae: 0.3465\n",
            "Epoch 38/256 loss: 0.2982 - mae: 0.3764 - val_loss: 0.2709 - val_mae: 0.3484\n",
            "Epoch 39/256 loss: 0.2988 - mae: 0.3747 - val_loss: 0.2692 - val_mae: 0.3468\n",
            "Epoch 40/256 loss: 0.2903 - mae: 0.3717 - val_loss: 0.2747 - val_mae: 0.3474\n",
            "Epoch 41/256 loss: 0.2949 - mae: 0.3745 - val_loss: 0.2728 - val_mae: 0.3597\n",
            "Epoch 42/256 loss: 0.2904 - mae: 0.3709 - val_loss: 0.2669 - val_mae: 0.3445\n",
            "Epoch 43/256 loss: 0.2880 - mae: 0.3713 - val_loss: 0.2686 - val_mae: 0.3465\n",
            "Epoch 44/256 loss: 0.2867 - mae: 0.3690 - val_loss: 0.2717 - val_mae: 0.3434\n",
            "Epoch 45/256 loss: 0.2912 - mae: 0.3704 - val_loss: 0.2639 - val_mae: 0.3358\n",
            "Epoch 46/256 loss: 0.2845 - mae: 0.3676 - val_loss: 0.2664 - val_mae: 0.3379\n",
            "Epoch 47/256 loss: 0.2886 - mae: 0.3695 - val_loss: 0.2683 - val_mae: 0.3431\n",
            "Epoch 48/256 loss: 0.2852 - mae: 0.3691 - val_loss: 0.2622 - val_mae: 0.3321\n",
            "Epoch 49/256 loss: 0.2812 - mae: 0.3655 - val_loss: 0.2642 - val_mae: 0.3415\n",
            "Epoch 50/256 loss: 0.2786 - mae: 0.3645 - val_loss: 0.2671 - val_mae: 0.3473\n",
            "Epoch 51/256 loss: 0.2813 - mae: 0.3656 - val_loss: 0.2662 - val_mae: 0.3382\n",
            "Epoch 52/256 loss: 0.2803 - mae: 0.3658 - val_loss: 0.2598 - val_mae: 0.3351\n",
            "Epoch 53/256 loss: 0.2777 - mae: 0.3633 - val_loss: 0.2630 - val_mae: 0.3347\n",
            "Epoch 54/256 loss: 0.2846 - mae: 0.3641 - val_loss: 0.2632 - val_mae: 0.3427\n",
            "Epoch 55/256 loss: 0.2799 - mae: 0.3638 - val_loss: 0.2661 - val_mae: 0.3398\n",
            "Epoch 56/256 loss: 0.2789 - mae: 0.3634 - val_loss: 0.2583 - val_mae: 0.3341\n",
            "Epoch 57/256 loss: 0.2776 - mae: 0.3625 - val_loss: 0.2607 - val_mae: 0.3402\n",
            "Epoch 58/256 loss: 0.2796 - mae: 0.3655 - val_loss: 0.2655 - val_mae: 0.3390\n",
            "Epoch 59/256 loss: 0.2735 - mae: 0.3585 - val_loss: 0.2584 - val_mae: 0.3353\n",
            "Epoch 60/256 loss: 0.2734 - mae: 0.3588 - val_loss: 0.2870 - val_mae: 0.3338\n",
            "Epoch 61/256 loss: 0.2739 - mae: 0.3617 - val_loss: 0.2555 - val_mae: 0.3352\n",
            "Epoch 62/256 loss: 0.2764 - mae: 0.3603 - val_loss: 0.2747 - val_mae: 0.3445\n",
            "Epoch 63/256 loss: 0.2711 - mae: 0.3584 - val_loss: 0.2653 - val_mae: 0.3388\n",
            "Epoch 64/256 loss: 0.2716 - mae: 0.3582 - val_loss: 0.2591 - val_mae: 0.3413\n",
            "Epoch 65/256 loss: 0.2744 - mae: 0.3613 - val_loss: 0.2636 - val_mae: 0.3320\n",
            "Epoch 66/256 loss: 0.2673 - mae: 0.3557 - val_loss: 0.2640 - val_mae: 0.3358\n",
            "Epoch 67/256 loss: 0.2714 - mae: 0.3575 - val_loss: 0.2539 - val_mae: 0.3300\n",
            "Epoch 68/256 loss: 0.2678 - mae: 0.3573 - val_loss: 0.2599 - val_mae: 0.3343\n",
            "Epoch 69/256 loss: 0.2693 - mae: 0.3570 - val_loss: 0.2586 - val_mae: 0.3359\n",
            "Epoch 70/256 loss: 0.2696 - mae: 0.3586 - val_loss: 0.2688 - val_mae: 0.3502\n",
            "Epoch 71/256 loss: 0.2690 - mae: 0.3566 - val_loss: 0.2610 - val_mae: 0.3368\n",
            "Epoch 72/256 loss: 0.2652 - mae: 0.3550 - val_loss: 0.2587 - val_mae: 0.3387\n",
            "Epoch 73/256 loss: 0.2645 - mae: 0.3539 - val_loss: 0.2586 - val_mae: 0.3325\n",
            "Epoch 74/256 loss: 0.2653 - mae: 0.3542 - val_loss: 0.2547 - val_mae: 0.3340\n",
            "Epoch 75/256 loss: 0.2661 - mae: 0.3555 - val_loss: 0.2532 - val_mae: 0.3369\n",
            "Epoch 76/256 loss: 0.2648 - mae: 0.3536 - val_loss: 0.2594 - val_mae: 0.3331\n",
            "Epoch 77/256 loss: 0.2654 - mae: 0.3554 - val_loss: 0.2552 - val_mae: 0.3290\n",
            "Epoch 78/256 loss: 0.2644 - mae: 0.3533 - val_loss: 0.2556 - val_mae: 0.3372\n",
            "Epoch 79/256 loss: 0.2628 - mae: 0.3545 - val_loss: 0.2562 - val_mae: 0.3386\n",
            "Epoch 80/256 loss: 0.2622 - mae: 0.3536 - val_loss: 0.2540 - val_mae: 0.3259\n",
            "Epoch 81/256 loss: 0.2615 - mae: 0.3520 - val_loss: 0.2597 - val_mae: 0.3390\n",
            "Epoch 82/256 loss: 0.2661 - mae: 0.3549 - val_loss: 0.2511 - val_mae: 0.3280\n",
            "Epoch 83/256 loss: 0.2658 - mae: 0.3552 - val_loss: 0.2560 - val_mae: 0.3318\n",
            "Epoch 84/256 loss: 0.2594 - mae: 0.3519 - val_loss: 0.2570 - val_mae: 0.3307\n",
            "Epoch 85/256 loss: 0.2573 - mae: 0.3496 - val_loss: 0.2553 - val_mae: 0.3302\n",
            "Epoch 86/256 loss: 0.2588 - mae: 0.3505 - val_loss: 0.2498 - val_mae: 0.3262\n",
            "Epoch 87/256 loss: 0.2564 - mae: 0.3493 - val_loss: 0.2582 - val_mae: 0.3403\n",
            "Epoch 88/256 loss: 0.2611 - mae: 0.3528 - val_loss: 0.2557 - val_mae: 0.3338\n",
            "Epoch 89/256 loss: 0.2595 - mae: 0.3528 - val_loss: 0.2623 - val_mae: 0.3280\n",
            "Epoch 90/256 loss: 0.2553 - mae: 0.3485 - val_loss: 0.2555 - val_mae: 0.3362\n",
            "Epoch 91/256 loss: 0.2576 - mae: 0.3506 - val_loss: 0.2595 - val_mae: 0.3389\n",
            "Epoch 92/256 loss: 0.2573 - mae: 0.3512 - val_loss: 0.2550 - val_mae: 0.3281\n",
            "Epoch 93/256 loss: 0.2588 - mae: 0.3501 - val_loss: 0.2556 - val_mae: 0.3305\n",
            "Epoch 94/256 loss: 0.2521 - mae: 0.3472 - val_loss: 0.2509 - val_mae: 0.3327\n",
            "Epoch 95/256 loss: 0.2584 - mae: 0.3519 - val_loss: 0.2535 - val_mae: 0.3362\n",
            "Epoch 96/256 loss: 0.2544 - mae: 0.3469 - val_loss: 0.2493 - val_mae: 0.3302\n",
            "Epoch 97/256 loss: 0.2549 - mae: 0.3481 - val_loss: 0.2587 - val_mae: 0.3406\n",
            "Epoch 98/256 loss: 0.2530 - mae: 0.3475 - val_loss: 0.2567 - val_mae: 0.3377\n",
            "Epoch 99/256 loss: 0.2543 - mae: 0.3485 - val_loss: 0.2516 - val_mae: 0.3305\n",
            "Epoch 100/256 loss: 0.2512 - mae: 0.3474 - val_loss: 0.2508 - val_mae: 0.3294\n",
            "Epoch 101/256 loss: 0.2494 - mae: 0.3458 - val_loss: 0.2649 - val_mae: 0.3415\n",
            "Epoch 102/256 loss: 0.2523 - mae: 0.3469 - val_loss: 0.2515 - val_mae: 0.3308\n",
            "Epoch 103/256 loss: 0.2512 - mae: 0.3459 - val_loss: 0.2518 - val_mae: 0.3281\n",
            "Epoch 104/256 loss: 0.2551 - mae: 0.3480 - val_loss: 0.2501 - val_mae: 0.3331\n",
            "Epoch 105/256 loss: 0.2518 - mae: 0.3458 - val_loss: 0.2544 - val_mae: 0.3332\n",
            "Epoch 106/256 loss: 0.2505 - mae: 0.3453 - val_loss: 0.2509 - val_mae: 0.3309\n",
            "Epoch 107/256 loss: 0.2515 - mae: 0.3479 - val_loss: 0.2518 - val_mae: 0.3332\n",
            "Epoch 108/256 loss: 0.2500 - mae: 0.3455 - val_loss: 0.2532 - val_mae: 0.3274\n",
            "Epoch 109/256 loss: 0.2477 - mae: 0.3440 - val_loss: 0.2508 - val_mae: 0.3249\n",
            "Epoch 110/256 loss: 0.2493 - mae: 0.3447 - val_loss: 0.2559 - val_mae: 0.3365\n",
            "Epoch 111/256 loss: 0.2527 - mae: 0.3475 - val_loss: 0.2530 - val_mae: 0.3293\n",
            "Epoch 112/256 loss: 0.2492 - mae: 0.3435 - val_loss: 0.2485 - val_mae: 0.3308\n",
            "Epoch 113/256 loss: 0.2508 - mae: 0.3469 - val_loss: 0.2671 - val_mae: 0.3434\n",
            "Epoch 114/256 loss: 0.2517 - mae: 0.3458 - val_loss: 0.2464 - val_mae: 0.3262\n",
            "Epoch 115/256 loss: 0.2490 - mae: 0.3454 - val_loss: 0.2675 - val_mae: 0.3380\n",
            "Epoch 116/256 loss: 0.2479 - mae: 0.3444 - val_loss: 0.2488 - val_mae: 0.3265\n",
            "Epoch 117/256 loss: 0.2475 - mae: 0.3429 - val_loss: 0.2525 - val_mae: 0.3363\n",
            "Epoch 118/256 loss: 0.2507 - mae: 0.3455 - val_loss: 0.2509 - val_mae: 0.3310\n",
            "Epoch 119/256 loss: 0.2459 - mae: 0.3433 - val_loss: 0.2502 - val_mae: 0.3330\n",
            "Epoch 120/256 loss: 0.2443 - mae: 0.3417 - val_loss: 0.2484 - val_mae: 0.3270\n",
            "Epoch 121/256 loss: 0.2508 - mae: 0.3455 - val_loss: 0.2490 - val_mae: 0.3257\n",
            "Epoch 122/256 loss: 0.2451 - mae: 0.3427 - val_loss: 0.2503 - val_mae: 0.3309\n",
            "Epoch 123/256 loss: 0.2476 - mae: 0.3429 - val_loss: 0.2552 - val_mae: 0.3346\n",
            "Epoch 124/256 loss: 0.2448 - mae: 0.3421 - val_loss: 0.2547 - val_mae: 0.3288\n",
            "Epoch 125/256 loss: 0.2484 - mae: 0.3428 - val_loss: 0.2468 - val_mae: 0.3278\n",
            "Epoch 126/256 loss: 0.2451 - mae: 0.3430 - val_loss: 0.2448 - val_mae: 0.3263\n",
            "Epoch 127/256 loss: 0.2444 - mae: 0.3425 - val_loss: 0.2467 - val_mae: 0.3257\n",
            "Epoch 128/256 loss: 0.2404 - mae: 0.3398 - val_loss: 0.2495 - val_mae: 0.3292\n",
            "Epoch 129/256 loss: 0.2440 - mae: 0.3426 - val_loss: 0.2472 - val_mae: 0.3261\n",
            "Epoch 130/256 loss: 0.2422 - mae: 0.3414 - val_loss: 0.2454 - val_mae: 0.3252\n",
            "Epoch 131/256 loss: 0.2419 - mae: 0.3410 - val_loss: 0.2482 - val_mae: 0.3287\n",
            "Epoch 132/256 loss: 0.2406 - mae: 0.3407 - val_loss: 0.2471 - val_mae: 0.3233\n",
            "Epoch 133/256 loss: 0.2381 - mae: 0.3388 - val_loss: 0.2570 - val_mae: 0.3320\n",
            "Epoch 134/256 loss: 0.2427 - mae: 0.3410 - val_loss: 0.2470 - val_mae: 0.3249\n",
            "Epoch 135/256 loss: 0.2434 - mae: 0.3413 - val_loss: 0.2471 - val_mae: 0.3250\n",
            "Epoch 136/256 loss: 0.2442 - mae: 0.3416 - val_loss: 0.2487 - val_mae: 0.3334\n",
            "Epoch 137/256 loss: 0.2419 - mae: 0.3418 - val_loss: 0.2461 - val_mae: 0.3265\n",
            "Epoch 138/256 loss: 0.2443 - mae: 0.3414 - val_loss: 0.2447 - val_mae: 0.3242\n",
            "Epoch 139/256 loss: 0.2369 - mae: 0.3385 - val_loss: 0.2464 - val_mae: 0.3278\n",
            "Epoch 140/256 loss: 0.2422 - mae: 0.3413 - val_loss: 0.2471 - val_mae: 0.3230\n",
            "Epoch 141/256 loss: 0.2434 - mae: 0.3416 - val_loss: 0.2449 - val_mae: 0.3219\n",
            "Epoch 142/256 loss: 0.2370 - mae: 0.3377 - val_loss: 0.2483 - val_mae: 0.3283\n",
            "Epoch 143/256 loss: 0.2418 - mae: 0.3404 - val_loss: 0.2480 - val_mae: 0.3248\n",
            "Epoch 144/256 loss: 0.2380 - mae: 0.3380 - val_loss: 0.2462 - val_mae: 0.3243\n",
            "Epoch 145/256 loss: 0.2428 - mae: 0.3406 - val_loss: 0.2466 - val_mae: 0.3272\n",
            "Epoch 146/256 loss: 0.2382 - mae: 0.3385 - val_loss: 0.2522 - val_mae: 0.3297\n",
            "Epoch 147/256 loss: 0.2373 - mae: 0.3372 - val_loss: 0.2502 - val_mae: 0.3309\n",
            "Epoch 148/256 loss: 0.2395 - mae: 0.3396 - val_loss: 0.2479 - val_mae: 0.3240\n",
            "Epoch 149/256 loss: 0.2383 - mae: 0.3382 - val_loss: 0.2493 - val_mae: 0.3278\n",
            "Epoch 150/256 loss: 0.2374 - mae: 0.3385 - val_loss: 0.2527 - val_mae: 0.3326\n",
            "Epoch 151/256 loss: 0.2422 - mae: 0.3393 - val_loss: 0.2429 - val_mae: 0.3241\n",
            "Epoch 152/256 loss: 0.2389 - mae: 0.3394 - val_loss: 0.2506 - val_mae: 0.3246\n",
            "Epoch 153/256 loss: 0.2402 - mae: 0.3392 - val_loss: 0.2415 - val_mae: 0.3204\n",
            "Epoch 154/256 loss: 0.2365 - mae: 0.3373 - val_loss: 0.2480 - val_mae: 0.3274\n",
            "Epoch 155/256 loss: 0.2391 - mae: 0.3407 - val_loss: 0.2450 - val_mae: 0.3270\n",
            "Epoch 156/256 loss: 0.2385 - mae: 0.3369 - val_loss: 0.2443 - val_mae: 0.3229\n",
            "Epoch 157/256 loss: 0.2380 - mae: 0.3383 - val_loss: 0.2427 - val_mae: 0.3189\n",
            "Epoch 158/256 loss: 0.2391 - mae: 0.3384 - val_loss: 0.2509 - val_mae: 0.3268\n",
            "Epoch 159/256 loss: 0.2327 - mae: 0.3365 - val_loss: 0.2431 - val_mae: 0.3268\n",
            "Epoch 160/256 loss: 0.2344 - mae: 0.3370 - val_loss: 0.2434 - val_mae: 0.3229\n",
            "Epoch 161/256 loss: 0.2381 - mae: 0.3382 - val_loss: 0.2461 - val_mae: 0.3286\n",
            "Epoch 162/256 loss: 0.2368 - mae: 0.3376 - val_loss: 0.2444 - val_mae: 0.3222\n",
            "Epoch 163/256 loss: 0.2372 - mae: 0.3385 - val_loss: 0.2444 - val_mae: 0.3197\n",
            "Epoch 164/256 loss: 0.2371 - mae: 0.3373 - val_loss: 0.2464 - val_mae: 0.3254\n",
            "Epoch 165/256 loss: 0.2359 - mae: 0.3362 - val_loss: 0.2474 - val_mae: 0.3258\n",
            "Epoch 166/256 loss: 0.2385 - mae: 0.3383 - val_loss: 0.2417 - val_mae: 0.3201\n",
            "Epoch 167/256 loss: 0.2348 - mae: 0.3378 - val_loss: 0.2458 - val_mae: 0.3259\n",
            "Epoch 168/256 loss: 0.2327 - mae: 0.3345 - val_loss: 0.2433 - val_mae: 0.3253\n",
            "Epoch 169/256 loss: 0.2373 - mae: 0.3375 - val_loss: 0.2406 - val_mae: 0.3195\n",
            "Epoch 170/256 loss: 0.2346 - mae: 0.3369 - val_loss: 0.2435 - val_mae: 0.3229\n",
            "Epoch 171/256 loss: 0.2325 - mae: 0.3344 - val_loss: 0.2439 - val_mae: 0.3205\n",
            "Epoch 172/256 loss: 0.2370 - mae: 0.3393 - val_loss: 0.2489 - val_mae: 0.3233\n",
            "Epoch 173/256 loss: 0.2303 - mae: 0.3334 - val_loss: 0.2441 - val_mae: 0.3228\n",
            "Epoch 174/256 loss: 0.2308 - mae: 0.3347 - val_loss: 0.2522 - val_mae: 0.3349\n",
            "Epoch 175/256 loss: 0.2368 - mae: 0.3384 - val_loss: 0.2484 - val_mae: 0.3270\n",
            "Epoch 176/256 loss: 0.2314 - mae: 0.3333 - val_loss: 0.2435 - val_mae: 0.3267\n",
            "Epoch 177/256 loss: 0.2313 - mae: 0.3347 - val_loss: 0.2402 - val_mae: 0.3193\n",
            "Epoch 178/256 loss: 0.2345 - mae: 0.3359 - val_loss: 0.2445 - val_mae: 0.3254\n",
            "Epoch 179/256 loss: 0.2337 - mae: 0.3356 - val_loss: 0.2425 - val_mae: 0.3243\n",
            "Epoch 180/256 loss: 0.2317 - mae: 0.3353 - val_loss: 0.2442 - val_mae: 0.3225\n",
            "Epoch 181/256 loss: 0.2348 - mae: 0.3359 - val_loss: 0.2390 - val_mae: 0.3197\n",
            "Epoch 182/256 loss: 0.2326 - mae: 0.3350 - val_loss: 0.2402 - val_mae: 0.3236\n",
            "Epoch 183/256 loss: 0.2323 - mae: 0.3363 - val_loss: 0.2409 - val_mae: 0.3235\n",
            "Epoch 184/256 loss: 0.2298 - mae: 0.3328 - val_loss: 0.2417 - val_mae: 0.3270\n",
            "Epoch 185/256 loss: 0.2272 - mae: 0.3317 - val_loss: 0.2405 - val_mae: 0.3209\n",
            "Epoch 186/256 loss: 0.2307 - mae: 0.3336 - val_loss: 0.2459 - val_mae: 0.3249\n",
            "Epoch 187/256 loss: 0.2311 - mae: 0.3331 - val_loss: 0.2414 - val_mae: 0.3255\n",
            "Epoch 188/256 loss: 0.2295 - mae: 0.3330 - val_loss: 0.2484 - val_mae: 0.3241\n",
            "Epoch 189/256 loss: 0.2372 - mae: 0.3373 - val_loss: 0.2431 - val_mae: 0.3270\n",
            "Epoch 190/256 loss: 0.2354 - mae: 0.3375 - val_loss: 0.2448 - val_mae: 0.3228\n",
            "Epoch 191/256 loss: 0.2293 - mae: 0.3332 - val_loss: 0.2567 - val_mae: 0.3285\n",
            "Epoch 192/256 loss: 0.2311 - mae: 0.3352 - val_loss: 0.2455 - val_mae: 0.3289\n",
            "Epoch 193/256 loss: 0.2295 - mae: 0.3322 - val_loss: 0.2463 - val_mae: 0.3232\n",
            "Epoch 194/256 loss: 0.2311 - mae: 0.3346 - val_loss: 0.2445 - val_mae: 0.3208\n",
            "Epoch 195/256 loss: 0.2290 - mae: 0.3339 - val_loss: 0.2436 - val_mae: 0.3266\n",
            "Epoch 196/256 loss: 0.2268 - mae: 0.3306 - val_loss: 0.2442 - val_mae: 0.3256\n",
            "Epoch 197/256 loss: 0.2276 - mae: 0.3318 - val_loss: 0.2435 - val_mae: 0.3254\n",
            "Epoch 198/256 loss: 0.2278 - mae: 0.3337 - val_loss: 0.2410 - val_mae: 0.3225\n",
            "Epoch 199/256 loss: 0.2298 - mae: 0.3326 - val_loss: 0.2434 - val_mae: 0.3265\n",
            "Epoch 200/256 loss: 0.2266 - mae: 0.3332 - val_loss: 0.2400 - val_mae: 0.3214\n",
            "Epoch 201/256 loss: 0.2276 - mae: 0.3323 - val_loss: 0.2432 - val_mae: 0.3217\n",
            "Epoch 202/256 loss: 0.2255 - mae: 0.3315 - val_loss: 0.2464 - val_mae: 0.3210\n",
            "Epoch 203/256 loss: 0.2266 - mae: 0.3311 - val_loss: 0.2415 - val_mae: 0.3207\n",
            "Epoch 204/256 loss: 0.2273 - mae: 0.3335 - val_loss: 0.2411 - val_mae: 0.3212\n",
            "Epoch 205/256 loss: 0.2261 - mae: 0.3310 - val_loss: 0.2451 - val_mae: 0.3284\n",
            "Epoch 206/256 loss: 0.2268 - mae: 0.3311 - val_loss: 0.2440 - val_mae: 0.3214\n",
            "Epoch 207/256 loss: 0.2250 - mae: 0.3303 - val_loss: 0.2424 - val_mae: 0.3207\n",
            "Epoch 208/256 loss: 0.2277 - mae: 0.3321 - val_loss: 0.2414 - val_mae: 0.3209\n",
            "Epoch 209/256 loss: 0.2273 - mae: 0.3328 - val_loss: 0.2410 - val_mae: 0.3228\n",
            "Epoch 210/256 loss: 0.2246 - mae: 0.3313 - val_loss: 0.2394 - val_mae: 0.3197\n",
            "Epoch 211/256 loss: 0.2296 - mae: 0.3352 - val_loss: 0.2397 - val_mae: 0.3233\n",
            "Epoch 212/256 loss: 0.2270 - mae: 0.3322 - val_loss: 0.2464 - val_mae: 0.3227\n",
            "Epoch 213/256 loss: 0.2262 - mae: 0.3314 - val_loss: 0.2427 - val_mae: 0.3213\n",
            "Epoch 214/256 loss: 0.2254 - mae: 0.3309 - val_loss: 0.2457 - val_mae: 0.3239\n",
            "Epoch 215/256 loss: 0.2258 - mae: 0.3293 - val_loss: 0.2437 - val_mae: 0.3242\n",
            "Epoch 216/256 loss: 0.2240 - mae: 0.3302 - val_loss: 0.2430 - val_mae: 0.3308\n",
            "Epoch 217/256 loss: 0.2251 - mae: 0.3331 - val_loss: 0.2395 - val_mae: 0.3208\n",
            "Epoch 218/256 loss: 0.2221 - mae: 0.3287 - val_loss: 0.2425 - val_mae: 0.3235\n",
            "Epoch 219/256 loss: 0.2252 - mae: 0.3314 - val_loss: 0.2581 - val_mae: 0.3254\n",
            "Epoch 220/256 loss: 0.2279 - mae: 0.3324 - val_loss: 0.2463 - val_mae: 0.3257\n",
            "Epoch 221/256 loss: 0.2269 - mae: 0.3309 - val_loss: 0.2424 - val_mae: 0.3226\n",
            "Epoch 222/256 loss: 0.2277 - mae: 0.3313 - val_loss: 0.2434 - val_mae: 0.3248\n",
            "Epoch 223/256 loss: 0.2294 - mae: 0.3332 - val_loss: 0.2397 - val_mae: 0.3177\n",
            "Epoch 224/256 loss: 0.2245 - mae: 0.3300 - val_loss: 0.2449 - val_mae: 0.3287\n",
            "Epoch 225/256 loss: 0.2259 - mae: 0.3313 - val_loss: 0.2408 - val_mae: 0.3222\n",
            "Epoch 226/256 loss: 0.2255 - mae: 0.3311 - val_loss: 0.2387 - val_mae: 0.3176\n",
            "Epoch 227/256 loss: 0.2228 - mae: 0.3302 - val_loss: 0.2399 - val_mae: 0.3164\n",
            "Epoch 228/256 loss: 0.2218 - mae: 0.3294 - val_loss: 0.2391 - val_mae: 0.3210\n",
            "Epoch 229/256 loss: 0.2220 - mae: 0.3286 - val_loss: 0.2408 - val_mae: 0.3240\n",
            "Epoch 230/256 loss: 0.2260 - mae: 0.3336 - val_loss: 0.2463 - val_mae: 0.3234\n",
            "Epoch 231/256 loss: 0.2223 - mae: 0.3303 - val_loss: 0.2381 - val_mae: 0.3180\n",
            "Epoch 232/256 loss: 0.2241 - mae: 0.3313 - val_loss: 0.2427 - val_mae: 0.3223\n",
            "Epoch 233/256 loss: 0.2216 - mae: 0.3276 - val_loss: 0.2416 - val_mae: 0.3213\n",
            "Epoch 234/256 loss: 0.2234 - mae: 0.3293 - val_loss: 0.2392 - val_mae: 0.3211\n",
            "Epoch 235/256 loss: 0.2233 - mae: 0.3295 - val_loss: 0.2432 - val_mae: 0.3218\n",
            "Epoch 236/256 loss: 0.2283 - mae: 0.3309 - val_loss: 0.2409 - val_mae: 0.3241\n",
            "Epoch 237/256 loss: 0.2242 - mae: 0.3297 - val_loss: 0.2431 - val_mae: 0.3251\n",
            "Epoch 238/256 loss: 0.2246 - mae: 0.3303 - val_loss: 0.2384 - val_mae: 0.3198\n",
            "Epoch 239/256 loss: 0.2225 - mae: 0.3295 - val_loss: 0.2394 - val_mae: 0.3205\n",
            "Epoch 240/256 loss: 0.2213 - mae: 0.3287 - val_loss: 0.2388 - val_mae: 0.3167\n",
            "Epoch 241/256 loss: 0.2229 - mae: 0.3295 - val_loss: 0.2429 - val_mae: 0.3231\n",
            "Epoch 242/256 loss: 0.2227 - mae: 0.3291 - val_loss: 0.2454 - val_mae: 0.3211\n",
            "Epoch 243/256 loss: 0.2243 - mae: 0.3309 - val_loss: 0.2473 - val_mae: 0.3211\n",
            "Epoch 244/256 loss: 0.2245 - mae: 0.3294 - val_loss: 0.2393 - val_mae: 0.3229\n",
            "Epoch 245/256 loss: 0.2218 - mae: 0.3288 - val_loss: 0.2389 - val_mae: 0.3234\n",
            "Epoch 246/256 loss: 0.2209 - mae: 0.3276 - val_loss: 0.2381 - val_mae: 0.3125\n",
            "Epoch 247/256 loss: 0.2170 - mae: 0.3261 - val_loss: 0.2426 - val_mae: 0.3222\n",
            "Epoch 248/256 loss: 0.2216 - mae: 0.3272 - val_loss: 0.2383 - val_mae: 0.3197\n",
            "Epoch 249/256 loss: 0.2177 - mae: 0.3262 - val_loss: 0.2394 - val_mae: 0.3210\n",
            "Epoch 250/256 loss: 0.2199 - mae: 0.3279 - val_loss: 0.2380 - val_mae: 0.3196\n",
            "Epoch 251/256 loss: 0.2181 - mae: 0.3262 - val_loss: 0.2395 - val_mae: 0.3250\n",
            "Epoch 252/256 loss: 0.2185 - mae: 0.3262 - val_loss: 0.2398 - val_mae: 0.3221\n",
            "Epoch 253/256 loss: 0.2251 - mae: 0.3298 - val_loss: 0.2387 - val_mae: 0.3240\n",
            "Epoch 254/256 loss: 0.2214 - mae: 0.3285 - val_loss: 0.2378 - val_mae: 0.3179\n",
            "Epoch 255/256 loss: 0.2197 - mae: 0.3254 - val_loss: 0.2379 - val_mae: 0.3167\n",
            "Epoch 256/256 loss: 0.2193 - mae: 0.3258 - val_loss: 0.2391 - val_mae: 0.3223\n",
            "Prediction: 1.42 , true value: 1.37\n",
            "Prediction: 2.53 , true value: 2.41\n",
            "Prediction: 1.18 , true value: 2.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this training, that the loss is now down to 0.219, and similarly, the validation loss is at 0.239, And we can also see that the predictions here are probably a little bit closer to the true values. So we can see there how we, by **making a more complex network with more layers and more neurons per layer, we got a little bit better results from the network.**"
      ],
      "metadata": {
        "id": "3wahJhiTnxrP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can continue to experiment with this. One thing you could do is you could try to remove the dropout layers and see if, what I would expect, then you'll see that the training loss would go down and be significantly lower, while the validation loss probably wouldn't, so you would basically have more overfitting of your network at that case."
      ],
      "metadata": {
        "id": "iSnVm87foFsc"
      }
    }
  ]
}